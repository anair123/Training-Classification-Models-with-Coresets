{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and train logistic regression\n",
    "## Purpose\n",
    "In this example we will demonstrate how to:\n",
    "\n",
    "   - Build a Coreset tree for logistic regression on a train dataset.\n",
    "   - Retrieve the root Coreset from the Coreset tree using get_coreset and train a logistic regression model on it\n",
    "   - Compare the model's quality to a model build on the entire dataset\n",
    "   - Add to the Coreset tree additional data through partial_build\n",
    "   - Train a model directly on the Coreset tree using the fit function\n",
    "   - Compare again the model's quality to a model build on the entire dataset\n",
    "\n",
    "In this example we'll be using the well-known Covertype Dataset (https://archive.ics.uci.edu/ml/datasets/covertype). We will split the data to three parts:\n",
    "   - train_1 - 50% of the data\n",
    "   - train_2 - 20% of the data\n",
    "   - test - 30% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from dataheroes import CoresetTreeServiceDTC\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, balanced_accuracy_score\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the training data: (464809, 54)\n"
     ]
    }
   ],
   "source": [
    "X, y = fetch_covtype(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f'Dimensions of the training data: {X_train.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the training and test data\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464809"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8296036929211656 0.9867050962430965 0.8710532430315913\n"
     ]
    }
   ],
   "source": [
    "full_dataset_model = xgb.XGBClassifier(random_state=42)\n",
    "full_dataset_model.fit(X_train, y_train)\n",
    "y_pred_full = full_dataset_model.predict(X_test)\n",
    "n_samples_full = X_train.shape[0]\n",
    "\n",
    "full_balanced = balanced_accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(f'Balanced Accuracy Score: {full_balanced}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.2 s ± 1.79 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "full_dataset_model = LogisticRegression().fit(X_full, y_full)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with a Coreset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataheroes.services.coreset_tree.dtc.CoresetTreeServiceDTC at 0x2119332c640>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the coreset tree\n",
    "service_obj = CoresetTreeServiceDTC(\n",
    "                                   optimized_for='training',\n",
    "                                   n_classes=7,\n",
    "                                   n_instances=X_train.shape[0]\n",
    "                                  )\n",
    "service_obj.build(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coreset balanced score (31,018 samples): 0.8195187814437122\n",
      "Coreset AUC score (31,018 samples): 0.978281428112\n",
      "Coreset f1-score (31,018 samples): 0.8371900897567188\n"
     ]
    }
   ],
   "source": [
    "# Ignore convergence warnings for logistic regression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Get the coreset\n",
    "coreset = service_obj.get_coreset(level=5) # level=5\n",
    "indices, X_train_coreset, y_train_coreset = coreset['data']\n",
    "w = coreset['w']\n",
    "# Train a logistic regression model on the coreset.\n",
    "coreset_model = xgb.XGBClassifier(random_state=42).fit(X_train_coreset, y_train_coreset, sample_weight=w)\n",
    "y_pred_coreset = coreset_model.predict(X_test)\n",
    "n_samples_coreset = y_train_coreset.shape[0]\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "coreset_score = balanced_accuracy_score(y_test, y_pred_coreset) # target: 0.8296036929211656\n",
    "\n",
    "\n",
    "print(f\"Balanced score: {coreset_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.39 s ± 326 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "coreset_model = LogisticRegression().fit(X_train_coreset, y_train_coreset, sample_weight=w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with a Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample balanced (68,856 samples): 0.7843310695650707\n",
      "sample AUC score (68,856 samples): 0.9820023882077724\n",
      "sample f1 score (68,856 samples): 0.8565871793327194\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "sample_length = 68856\n",
    "\n",
    "# Create a list of indices\n",
    "indices = list(range(X_train.shape[0]))\n",
    "\n",
    "# Get a random sample of indices\n",
    "random_indices = random.sample(indices, sample_length)\n",
    "\n",
    "# Retrieve elements from both arrays using the random indices\n",
    "X_train_sample = np.array([X_train[i] for i in random_indices])\n",
    "y_train_sample = np.array([y_train[i] for i in random_indices])\n",
    "\n",
    "# train the model with the sample\n",
    "sample_model = xgb.XGBClassifier(random_state=42).fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# evaluate the model\n",
    "sample_balanced = balanced_accuracy_score(y_test, sample_model.predict(X_test))\n",
    "\n",
    "print(f\"Balanced score): {sample_balanced}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "sample_model = xgb.XGBClassifier(random_state=42).fit(X_train_sample, y_train_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
